{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.12.12","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"none","dataSources":[],"dockerImageVersionId":31259,"isInternetEnabled":false,"language":"python","sourceType":"notebook","isGpuEnabled":false}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"../input/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true,"execution":{"iopub.status.busy":"2026-02-09T05:59:33.252400Z","iopub.execute_input":"2026-02-09T05:59:33.252755Z","iopub.status.idle":"2026-02-09T05:59:33.259341Z","shell.execute_reply.started":"2026-02-09T05:59:33.252730Z","shell.execute_reply":"2026-02-09T05:59:33.258172Z"}},"outputs":[],"execution_count":7},{"cell_type":"markdown","source":"# Evaluation metrics for regression","metadata":{}},{"cell_type":"markdown","source":"**Why do we need evaluation metrics**\n\nafter training a model ,we must answer one question:\n\nHow good are the predictions?\n\nevaluation metrics quantify prediction errors by comparing :\n\nActual values (y)\npredicted values(ycap)","metadata":{}},{"cell_type":"markdown","source":"**1) Mean Absolute Error(MAE):**\n\n    MAE is the average of absolute differences between actual and predicted values.\n\n\n\nKey properties:\n1) easy to understand\n2) same unit as target variable\n3) less sensitive to outliers\n\n\nwhen to use??\n\n   \n->>when all errors are equally important\n   ","metadata":{}},{"cell_type":"markdown","source":"**2)ROOT MEAN SQUARED ERROR(RMSE)**\n\nRMSE is the square root of the average of squared errors.\n\nsimple meaning\n\n->>\"Penalizes larger errors more strongly\"\n\n*Key properties::*\n\nSame unit as target variable \n\nsensitive to outliers \n\nAlways>=MAE\n\nwhen to use ??::\n\n  ->>when larger errors are costly\n\n\nR^2->MEASURES HOW WELL MODEL EXPLAINS VARIANCE IN DATA\n\n1.0->PERFECT MODEL\n\n\n\n\n","metadata":{}},{"cell_type":"code","source":"import numpy as np\nimport matplotlib.pyplot as plt\nimport pandas as pd","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2026-02-09T05:59:33.261113Z","iopub.execute_input":"2026-02-09T05:59:33.261672Z","iopub.status.idle":"2026-02-09T05:59:33.279233Z","shell.execute_reply.started":"2026-02-09T05:59:33.261641Z","shell.execute_reply":"2026-02-09T05:59:33.278163Z"}},"outputs":[],"execution_count":8},{"cell_type":"code","source":"from sklearn.metrics import mean_absolute_error,mean_squared_error,r2_score\n\nimport numpy as np\n\ny_true=np.array([50,60,70,80])\ny_pred=np.array([48,62,68,85])\n\nmae=mean_absolute_error(y_true,y_pred)\nrmse=mean_squared_error(y_true,y_pred,squared=False)\n\nr2=r2_score(y_true,y_pred)\n\nprint(\"MAE:\",mae)\nprint(\"rmse\",rmse)\nprint(\"r2 score\",r2)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2026-02-09T05:59:33.280288Z","iopub.execute_input":"2026-02-09T05:59:33.280628Z","iopub.status.idle":"2026-02-09T05:59:33.302386Z","shell.execute_reply.started":"2026-02-09T05:59:33.280592Z","shell.execute_reply":"2026-02-09T05:59:33.301171Z"}},"outputs":[{"traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)","\u001b[0;32m/tmp/ipykernel_55/358699720.py\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      7\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      8\u001b[0m \u001b[0mmae\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mmean_absolute_error\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0my_true\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0my_pred\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 9\u001b[0;31m \u001b[0mrmse\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mmean_squared_error\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0my_true\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0my_pred\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0msquared\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     10\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     11\u001b[0m \u001b[0mr2\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mr2_score\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0my_true\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0my_pred\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.12/dist-packages/sklearn/utils/_param_validation.py\u001b[0m in \u001b[0;36mwrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    192\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    193\u001b[0m             \u001b[0;31m# Map *args/**kwargs to the function signature\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 194\u001b[0;31m             \u001b[0mparams\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfunc_sig\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbind\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    195\u001b[0m             \u001b[0mparams\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mapply_defaults\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    196\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/lib/python3.12/inspect.py\u001b[0m in \u001b[0;36mbind\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   3278\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mthe\u001b[0m \u001b[0mpassed\u001b[0m \u001b[0marguments\u001b[0m \u001b[0mcan\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mbe\u001b[0m \u001b[0mbound\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3279\u001b[0m         \"\"\"\n\u001b[0;32m-> 3280\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_bind\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   3281\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3282\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mbind_partial\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m/\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/lib/python3.12/inspect.py\u001b[0m in \u001b[0;36m_bind\u001b[0;34m(self, args, kwargs, partial)\u001b[0m\n\u001b[1;32m   3267\u001b[0m                 )\n\u001b[1;32m   3268\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 3269\u001b[0;31m                 raise TypeError(\n\u001b[0m\u001b[1;32m   3270\u001b[0m                     'got an unexpected keyword argument {arg!r}'.format(\n\u001b[1;32m   3271\u001b[0m                         arg=next(iter(kwargs))))\n","\u001b[0;31mTypeError\u001b[0m: got an unexpected keyword argument 'squared'"],"ename":"TypeError","evalue":"got an unexpected keyword argument 'squared'","output_type":"error"}],"execution_count":9},{"cell_type":"code","source":"from sklearn.metrics import mean_absolute_error,mean_squared_error,r2_score\n\nimport numpy as np\n\ny_true=np.array([50,60,70,80])\ny_pred=np.array([48,62,68,85])\n\nmae=mean_absolute_error(y_true,y_pred)  \nmse=mean_squared_error(y_true,y_pred)  #mse  \n\nrmse=np.sqrt(mse) #rmse ->we used this because new versions of sckicit learn do not accept squared as an argument\n\nr2=r2_score(y_true,y_pred)\n\nprint(\"MAE:\",mae)\nprint(\"rmse\",rmse)\nprint(\"r2 score\",r2)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2026-02-09T05:59:38.791583Z","iopub.execute_input":"2026-02-09T05:59:38.791932Z","iopub.status.idle":"2026-02-09T05:59:38.801825Z","shell.execute_reply.started":"2026-02-09T05:59:38.791903Z","shell.execute_reply":"2026-02-09T05:59:38.800934Z"}},"outputs":[{"name":"stdout","text":"MAE: 2.75\nrmse 3.0413812651491097\nr2 score 0.926\n","output_type":"stream"}],"execution_count":10},{"cell_type":"code","source":"","metadata":{"trusted":true},"outputs":[],"execution_count":null}]}